defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps: []

y_sys:
  steps: []

train_sys:
  steps:

    # Fist Stage (Sampled Data)
    - _target_: src.modules.training.mixed_precision_trainer.MixedPrecisionTrainer
      model_name: ChembertaFreeze
      model:
        _target_: src.modules.training.models.chemberta.Chemberta
        n_classes: 3
        model_name: "DeepChem/ChemBERTa-10M-MTR"
      criterion:
        _target_: torch.nn.BCEWithLogitsLoss
      optimizer:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.AdamW
        lr: 0.001
      epochs: 8
      batch_size: 256
      patience: 8
      sample_size: 10_000_000
      dataset:
        _target_: src.modules.training.datasets.main_dataset.MainDataset
        retrieval: [ "SMILES_MOL" ]
        steps:
          - _target_: src.modules.training.dataset_steps.smiles.chemberta_tokenizer.ChembertaTokenizer
            padding_size: 120
            model_name: "DeepChem/ChemBERTa-10M-MTR"
      scheduler:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: timm.scheduler.cosine_lr.CosineLRScheduler
        t_initial: 100
        cycle_mul: 1
        cycle_decay: 1
        cycle_limit: 1
        # warmup_t: 1
        # warmup_lr_init: 1e-5
      n_folds: 5
      dataloader_args:
        num_workers: 16
        prefetch_factor: 2
        persistent_workers: true

    # Second Stage (All Data)
    - _target_: src.modules.training.main_trainer.MainTrainer
      model_name: ChembertaFreeze
      model:
        _target_: src.modules.training.models.chemberta.Chemberta
        n_classes: 3
        model_name: "DeepChem/ChemBERTa-10M-MTR"
      criterion:
        _target_: torch.nn.BCEWithLogitsLoss
      optimizer:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.AdamW
        lr: 0.001
      epochs: 3
      batch_size: 256
      # patience: 10
      dataset:
        _target_: src.modules.training.datasets.main_dataset.MainDataset
        retrieval: [ "SMILES_MOL" ]
        steps:
          - _target_: src.modules.training.dataset_steps.smiles.chemberta_tokenizer.ChembertaTokenizer
            padding_size: 120
            model_name: "DeepChem/ChemBERTa-10M-MTR"
      scheduler:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: timm.scheduler.cosine_lr.CosineLRScheduler
        t_initial: 100
        # cycle_mul: 1
        # cycle_decay: 1
        # cycle_limit: 1
        # warmup_t: 1
        # warmup_lr_init: 1e-5
      n_folds: 5
      dataloader_args:
        num_workers: 16
        prefetch_factor: 2
        persistent_workers: true

    # Prediction Statistics
    - _target_: src.modules.training.analysis.prediction_statistics.PredictionStatistics


pred_sys:
  steps:
    - _target_: src.modules.transformation.sigmoid.Sigmoid

label_sys:
  steps: []